<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">



<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

    <head>

        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

        <title>Yang Zhao</title>

        

        <!-- CSS -->

        <link href="css" rel="stylesheet" type="text/css">

        <link rel="stylesheet" href="style.css" type="text/css" media="screen">

        <!-- ENDS CSS -->

        <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>

        

        <!-- ENDS JS -->  

    </head> 

    <body>

        <!-- MAIN -->

        <div id="">

            <div id="match-nav-wrapper">

               <div id="match-nav-bar">

                     <table>

                        <thead>

                            <tr valign="bottom">

                                <th width="" style="font-size: 25px;  color: #dddddd">Yang Zhao</th>

                               <th width=40% ></th>

                                <th width=""><a href="index.html">HOME |</a> </th>

                                <th width=""><a href="publication.html"> PUBLICATIONS |</a></th>

                                <th width=""><a href="others.html">OTHERS</a></th>

                                <!-- <th width=""><a href="teaching.html"">DATASETS |</a></th> -->

                                <!-- <th width=""><a href="others.html">OTHERS</a></th> -->

                                <!-- <th width=""><a href="#" onclick="goto('about.html');">About</a></th> -->

                            </tr>

                        </thead>

                    </table>

                </div>

            </div>

            <!-- HEADER -->





            <div id="main-wrapper">



                <div id="portfolio-info">

                <!-- <h1>Publication</h1> -->

                    <table id="portfolio-projects">

                        <tbody>

                          <tr style="border-width: 1px">
                            <td><img src="./paperFig/SLANet.png" style="margin-bottom: 12px"></td>
                            <td style="">
                                <table style="width: 100%;">
                                    <tbody><tr>
                                            <td style="width: 100%; text-align: left;">
                                            <p>
                                    <b>Self-Supervised Lie Algebra Representation Learning via Optimal Canonical Metric. </b><br>
                                    Xiaohan Yu, Zicheng Pan, <b>Yang Zhao</b>, Yongsheng Gao<br>
                                     The IEEE Transactions on Neural Networks and Learning Systems, 2024. <br>
<!--                                          <a href="https://link.springer.com/article/10.1007/s10792-023-02669-3">Paper Link</a> -->
<!--                                     <a href="https://github.com/yangyangkiki/PR-ImprovedTripletloss/tree/master">Code</a> -->
                                            </p>
                                            </td>
                                <td style="width: 10px;">
                                </td>
                                </tr></tbody></table>
                            </td>
                          </tr>   

                          <tr style="border-width: 1px">
                            <td><img src="./paperFig/HFO.png" style="margin-bottom: 12px"></td>
                            <td bgcolor="#e4e4e4">
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                     <b> Occluded Person Retrieval with Hierarchical Feature Optimization.</b><br>
                                      <b>Yang Zhao</b>, Pengcheng Zhang, Xiaohan Yu, Zhibin Liao, Johan Verjans, Xiao Bai, Wei Xiang<br>
                                    18th IEEE International Conference on Automatic Face and Gesture Recognition (IEEE FG2024), 2024. <br>
<!--                                     <a href="https://ieeexplore.ieee.org/abstract/document/9953322">Paper Link</a>  -->
<!--                                     &nbsp;·&nbsp; -->
<!--                                     <a href="https://github.com/yangyangkiki/Gait-Assisted-Video-Reid">Code</a> -->

                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr>
                                </tbody></table>
                            </td>
                         </tr>
                            
                            
                          <tr style="border-width: 1px">
                            <td><img src="./paperFig/Int_Opht.png" style="margin-bottom: 12px"></td>
                            <td style="">
                                <table style="width: 100%;">
                                    <tbody><tr>
                                            <td style="width: 100%; text-align: left;">
                                            <p>
                                    <b>Normal periocular anthropometric measurements in an Australian population. </b><br>
                                    Khizar Rana, Mark B. Beecher, Carmelo Caltabiano, <b>Yang Zhao</b>, Johan Verjans, Dinesh Selva<br>
                                    International Ophthalmology, 2023. <br>
                                         <a href="https://link.springer.com/article/10.1007/s10792-023-02669-3">Paper Link</a>
<!--                                     <a href="https://github.com/yangyangkiki/PR-ImprovedTripletloss/tree/master">Code</a> -->
                                            </p>
                                            </td>
                                <td style="width: 10px;">
                                </td>
                                </tr></tbody></table>
                            </td>
                          </tr>   
                            
                          <tr style="border-width: 1px">
                            <td><img src="./paperFig/ATP.PNG" style="margin-bottom: 12px"></td>
                            <td bgcolor="#e4e4e4">
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                     <b> Learning Deep Asymmetric Tolerant Part Representation.</b><br>
                                      Xiaohan Yu, <b>Yang Zhao</b>, Yongsheng Gao, Shengwu Xiong<br>
                                    IEEE Transactions on Artificial Intelligence (IEEE TAI), 2022. <br>
                                    <a href="https://ieeexplore.ieee.org/abstract/document/9953322">Paper Link</a> 
<!--                                     &nbsp;·&nbsp; -->
<!--                                     <a href="https://github.com/yangyangkiki/Gait-Assisted-Video-Reid">Code</a> -->

                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr>
                                </tbody></table>
                            </td>
                         </tr>
                            
                          <tr style="border-width: 1px">
                            <td><img src="./paperFig/MixViT.PNG" style="margin-bottom: 12px"></td>
                            <td style="">
                                <table style="width: 100%;">
                                    <tbody><tr>
                                            <td style="width: 100%; text-align: left;">
                                            <p>
                                    <b>Mix-ViT: Mixing Attentive Vision Transformer for Ultra-Fine-Grained Visual Categorization. </b><br>
                                    Xiaohan Yu, Jun Wang, <b>Yang Zhao</b>, and Yongsheng Gao<br>
                                    Pattern Recognition (PR), 2023. <br>
                                         <a href="https://www.sciencedirect.com/science/article/pii/S0031320322006112">Paper Link</a>
<!--                                     <a href="https://github.com/yangyangkiki/PR-ImprovedTripletloss/tree/master">Code</a> -->
                                            </p>
                                            </td>
                                <td style="width: 10px;">
                                </td>
                                </tr></tbody></table>
                            </td>
                          </tr>
                            
                            
                          <tr style="border-width: 1px">
                            <td><img src="./paperFig/GaitReid.PNG" style="margin-bottom: 12px"></td>
                            <td bgcolor="#e4e4e4">
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                     <b> Gait-Assisted Video Person Retrieval.</b><br>
                                      <b>Yang Zhao</b>, Xinlong Wang, Xiaohan Yu, Chunlei Liu, Yongsheng Gao<br>
                                    IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT), 2022. <br>
                                    <a href="https://ieeexplore.ieee.org/document/9869674">Paper Link</a>  &nbsp;·&nbsp;
                                    <a href="https://github.com/yangyangkiki/Gait-Assisted-Video-Reid">Code</a>

                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr>
                                </tbody></table>
                            </td>
                         </tr>
                          
                          <tr style="border-width: 1px">
                            <td><img src="./paperFig/SPARE.png" style="margin-bottom: 12px"></td>
                            <td style="">
                                <table style="width: 100%;">
                                    <tbody><tr>
                                            <td style="width: 100%; text-align: left;">
                                            <p>
                                    <b>SPARE: Self-Supervised Part Erasing for Ultra-Fine-Grained Visual Categorization. </b><br>
                                    Xiaohan Yu, <b>Yang Zhao</b>, and Yongsheng Gao<br>
                                    Pattern Recognition (PR), 2022. <br>
                                         <a href="https://www.sciencedirect.com/science/article/pii/S0031320322001728?dgcid=rss_sd_all">Paper Link</a>
<!--                                     <a href="https://github.com/yangyangkiki/PR-ImprovedTripletloss/tree/master">Code</a> -->
                                            </p>
                                            </td>
                                <td style="width: 10px;">
                                </td>
                                </tr></tbody></table>
                            </td>
                          </tr>
                            
                            
                          <tr style="border-width: 1px">
                            <td><img src="./paperFig/RPC.png" style="margin-bottom: 12px"></td>
                            <td bgcolor="#e4e4e4">
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                     <b> RB-Net: Training Highly Accurate and Efficient Binary Neural Networks with Reshaped Point-wise Convolution and Balanced Activation.</b><br>
                                      Chunlei Liu, Wenrui Ding, Peng Chen, Bohan Zhuang, Yufeng Wang, <b>Yang Zhao</b>, Baochang Zhang, Yuqi Han<br>
                                    IEEE Transactions on Circuits and Systems for Video Technology (IEEE TCSVT), 2022. <br>
                                    <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9756000">Paper Link</a> 

                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr>
                                </tbody></table>
                            </td>
                         </tr>
                            
                            
                          <tr style="border-width: 1px">
                            <td><img src="./paperFig/retinal_seg.png" style="margin-bottom: 12px"></td>
                            <td style="">
                                <table style="width: 100%;">
                                    <tbody><tr>
                                            <td style="width: 100%; text-align: left;">
                                            <p>
                                    <b>EAR-NET: Error Attention Refining Network For Retinal Vessel Segmentation. </b><br>
                                    Jun Wang, <b>Yang Zhao</b>, Linglong Qian, Xiaohan Yu and Yongsheng Gao<br>
                                    Digital Image Computing: Techniques and Applications (DICTA), 2021. <br>
                                         <a href="https://arxiv.org/pdf/2107.01351.pdf">Paper Link</a>
<!--                                     <a href="https://github.com/yangyangkiki/PR-ImprovedTripletloss/tree/master">Code</a> -->
                                            </p>
                                            </td>
                                <td style="width: 10px;">
                                </td>
                                </tr></tbody></table>
                            </td>
                          </tr>
                            
                          <tr style="border-width: 1px">
                            <td><img src="./paperFig/HRS_Duke.png" style="margin-bottom: 12px"></td>
                            <td bgcolor="#e4e4e4">
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                     <b> Learning Discriminative Region Representation for Person Retrieval.</b><br>
                                      <b>Yang Zhao</b>, Xiaohan Yu, Yongsheng Gao and Chunhua Shen<br>
                                    Pattern Recognition (PR), 2021. <br>
                                    <a href="https://www.sciencedirect.com/science/article/pii/S0031320321004106">Paper Link</a> &nbsp;·&nbsp;
                                    <a href="https://github.com/yangyangkiki/Coseg-PersonReid">Code</a>
                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr>
                                </tbody></table>
                            </td>
                         </tr>
                            
                         <tr style="border-width: 1px">
                            <td><img src="./paperFig/ultra_fg.png" style="margin-bottom: 12px"></td>
                            <td style="">
                                <table style="width: 100%;">
                                    <tbody><tr>
                                            <td style="width: 100%; text-align: left;">
                                            <p>
                                    <b>Benchmark Platform for Ultra-Fine-Grained Visual Categorization Beyond Human Performance. </b><br>
                                    Xiaohan Yu, <b>Yang Zhao</b>, Yongsheng Gao, Xiaohui Yuan and Shengwu Xiong<br>
                                    In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. <br>
                                    <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_Benchmark_Platform_for_Ultra-Fine-Grained_Visual_Categorization_Beyond_Human_Performance_ICCV_2021_paper.pdf">Paper Link</a> &nbsp;·&nbsp; 
                                    <a href="https://github.com/XiaohanYu-GU/Ultra-FGVC">Dataset</a>
                                            </p>
                                            </td>
                                <td style="width: 10px;">
                                </td>
                                </tr></tbody></table>
                            </td>
                        </tr>

                         <tr style="border-width: 1px">
                            <td><img src="./paperFig/Ranmaskshuf.jpg" style="margin-bottom: 12px"></td>
                            <td bgcolor="#e4e4e4">
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                     <b> MaskCOV: A Random Mask Covariance Network for Ultra-Fine-Grained Visual Categorization.</b><br>
                                      Xiaohan Yu, <b>Yang Zhao</b>, Yongsheng Gao, and Shengwu Xiong<br>
                                    Pattern Recognition (PR), 2021. <br>
                                    <a href="https://www.sciencedirect.com/science/article/pii/S0031320321002545">Paper Link</a> &nbsp;·&nbsp;
                                    <a href="https://github.com/XiaohanYu-GU/MaskCOV">Code</a>
                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr>
                                </tbody></table>
                            </td>
                        </tr>

                       <tr style="border-width: 1px">
                            <td><img src="./paperFig/improved_loss.png" style="margin-bottom: 12px"></td>
                            <td style="">
                                <table style="width: 100%;">
                                    <tbody><tr>
                                            <td style="width: 100%; text-align: left;">
                                            <p>
                                    <b>Learning Deep Part-Aware Embedding for Person Retrieval. </b><br>
                                    <b>Yang Zhao</b>, Chunhua Shen, Xiaohan Yu, Hao Chen, Yongsheng Gao, and Shengwu Xiong<br>
                                    Pattern Recognition (PR), 2021. <br>
                                    <a href="https://www.sciencedirect.com/science/article/pii/S0031320321001254">Paper Link</a> &nbsp;·&nbsp;
                                    <a href="https://github.com/yangyangkiki/PR-ImprovedTripletloss/tree/master">Code</a>
                                            </p>
                                            </td>
                                <td style="width: 10px;">
                                </td>
                                </tr></tbody></table>
                            </td>
                        </tr>

                       <tr style="border-width: 1px">
                            <td><img src="./paperFig/HRNet_V2.png" style="margin-bottom: 12px"></td>
                            <td bgcolor="#e4e4e4">
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                     <b> Deep high-resolution representation learning for visual recognition.</b><br>
                                      Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, <b>Yang Zhao</b>, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu and Bin Xiao<br>
                                    IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE TPAMI), 2020. <br>
                                    <a href="https://ieeexplore.ieee.org/document/9052469?denied=">Paper Link</a> &nbsp;·&nbsp;
                                    <a href="https://github.com/HRNet">Code</a>
                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr>
                                </tbody></table>
                            </td>
                        </tr>
                            
                       <tr style="border-width: 1px">
                            <td><img src="./paperFig/face_lmk.png" style="margin-bottom: 12px"></td>
                            <td style="">
                                <table style="width: 100%;">
                                    <tbody><tr>
                                            <td style="width: 100%; text-align: left;">
                                            <p>
                                    <b>MobileFAN: transferring deep hidden representation for face alignment. </b><br>
                                    <b>Yang Zhao</b>, Yifan Liu, Chunhua Shen, Yongsheng Gao, and Shengwu Xiong<br>
                                    Pattern Recognition (PR), 2020. <br>
                                    <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320319304157">Paper Link</a> &nbsp;·&nbsp;
                                    <a href="https://github.com/yangyangkiki/mobilefan">Code</a>
                                            </p>
                                            </td>
                                <td style="width: 10px;">
                                </td>
                                </tr></tbody></table>           
                            </td>
                        </tr>  

                            

                        <tr style="border-width: 1px">
                            <td><img src="./paperFig/MORT_patchy.png" style="margin-bottom: 12px"></td>
                            <td bgcolor="#e4e4e4">
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                     <b> Patchy Image Structure Classification Using Multi-Orientation Region Transform.</b><br>
                                      Xiaohan Yu, <b>Yang Zhao</b>, Yongsheng Gao, Shengwu Xiong, and Xiaohui Yuan <br>
                                      In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2020<br>
                                      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6968">Paper Link</a> &nbsp;·&nbsp;
                                    <a href="https://github.com/XiaohanYu-GU/MReT2019/">Code</a>
                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr>
                                </tbody></table>           
                            </td>
                        </tr> 
                             
                        
                            <tr style="border-width: 1px">
                            <td><img src="./paperFig/hrnet_pose.png" style="margin-bottom: 12px"></td>
                            <td style="">
                                <table style="width: 100%;">
                                    <tbody><tr>
                                            <td style="width: 100%; text-align: left;">
                                            <p>
                                    <b>High-resolution representations for labeling pixels and regions. </b><br>
                                    Sun Ke*, <b>Yang Zhao</b>*, Borui Jiang*, Tianheng Cheng*, Bin Xiao, Dong Liu, Yadong Mu, Xinggang Wang, Wenyu Liu, and Jingdong Wang<br>
                                    ArXiv Preprint, 2019<br>
                                    <a href="https://arxiv.org/pdf/1904.04514.pdf%E3%80%80%E3%80%80">Paper Link</a>&nbsp;·&nbsp;
                                    <a href="https://github.com/HRNet/HRNet-Facial-Landmark-Detection">Code</a>
                                            </p>
                                            </td>
                                <td style="width: 10px;">
                                </td>
                                </tr></tbody></table>           
                            </td>
                        </tr>  
                                                                            

                            

            </div>

            </div>

        </div>

        <!-- ENDS MAIN -->  

        

  

 

    

</div></div></body></html>
